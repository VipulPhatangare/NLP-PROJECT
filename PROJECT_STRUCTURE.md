# ğŸ“ Project Structure

```
NLP Project/
â”‚
â”œâ”€â”€ ğŸ“„ package.json              # Node.js dependencies and scripts
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ server.js                 # Express server (main backend)
â”œâ”€â”€ ğŸ“„ config.py                 # Configuration settings
â”‚
â”œâ”€â”€ ğŸ“„ README.md                 # Detailed documentation
â”œâ”€â”€ ğŸ“„ QUICKSTART.md            # Quick start guide
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md     # This file
â”‚
â”œâ”€â”€ ğŸ”§ install.ps1              # Installation script
â”œâ”€â”€ ğŸ”§ start.ps1                # Startup script
â”œâ”€â”€ ğŸ“„ .gitignore               # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                 # Python processing scripts
â”‚   â”œâ”€â”€ scraper.py              # Web scraping (async)
â”‚   â”œâ”€â”€ phase1_preprocessing.py # Text preprocessing pipeline
â”‚   â””â”€â”€ phase2_analysis.py      # NLP analysis pipeline
â”‚
â”œâ”€â”€ ğŸ“‚ public/                  # Frontend files (served by Express)
â”‚   â”œâ”€â”€ index.html              # Main dashboard UI
â”‚   â”œâ”€â”€ styles.css              # Styling and layout
â”‚   â””â”€â”€ app.js                  # Frontend JavaScript logic
â”‚
â”œâ”€â”€ ğŸ“‚ data/                    # Data storage (created at runtime)
â”‚   â”œâ”€â”€ flipkart_boat_raw.csv   # Raw scraped reviews
â”‚   â””â”€â”€ flipkart_boat_cleaned.csv # Preprocessed reviews
â”‚
â””â”€â”€ ğŸ“‚ results/                 # Analysis results (created at runtime)
    â””â”€â”€ phase2_results.json     # Phase 2 analysis output
```

## ğŸ“ File Descriptions

### Root Files

**package.json**
- Defines Node.js project metadata
- Lists Express, CORS, and other Node dependencies
- Contains npm scripts (start, dev)

**requirements.txt**
- Python package dependencies
- Includes: requests-html, beautifulsoup4, nltk, gensim, scikit-learn, pandas
- Used with: `pip install -r requirements.txt`

**server.js**
- Express web server
- REST API endpoints for data processing
- Serves static files from public/
- Routes: /api/status, /api/scrape, /api/phase1, /api/phase2, /api/data/*

**config.py**
- Centralized configuration
- Scraping settings (URLs, selectors, timing)
- NLP parameters (TF-IDF, LSA, Word2Vec)
- Customizable without editing main scripts

### Scripts Directory

**scripts/scraper.py**
- Asynchronous web scraping using requests-html
- Batch processing with configurable delays
- Extracts: reviews, ratings, titles, reviewer names
- Output: CSV file with raw data
- Usage: `python scripts/scraper.py [pages] [batch_size] [delay]`

**scripts/phase1_preprocessing.py**
- Emoji to word mapping
- Language detection (langdetect)
- Translation (deep-translator)
- Text cleaning and normalization
- Tokenization, stopword removal, lemmatization
- Output: Cleaned CSV + statistics JSON
- Usage: `python scripts/phase1_preprocessing.py [input] [output]`

**scripts/phase2_analysis.py**
- POS tagging (NLTK)
- Named Entity Recognition (rule-based)
- Bag-of-Words & TF-IDF vectorization
- LSA topic modeling
- Word2Vec embeddings and similarity
- Output: Comprehensive JSON results
- Usage: `python scripts/phase2_analysis.py [input] [output_dir]`

### Public Directory

**public/index.html**
- Single-page dashboard application
- Tabs: Overview, Phase 1, Phase 2, Data View
- Status indicators for each processing phase
- Control panel with action buttons
- Responsive layout with modern design

**public/styles.css**
- Modern, responsive styling
- CSS variables for theming
- Card-based layout
- Chart containers
- Table styling
- Mobile-friendly breakpoints

**public/app.js**
- Frontend application logic
- API communication with backend
- Chart.js integration for visualizations
- Dynamic content rendering
- Tab navigation
- Data table generation

### Data and Results

**data/**
- Created automatically by server
- Stores raw and processed CSV files
- Gitignored (except sample data)

**results/**
- Created automatically by server
- Stores JSON analysis results
- Gitignored

## ğŸ”„ Data Flow

```
1. User clicks "Scrape Reviews"
   â†“
2. Frontend â†’ POST /api/scrape â†’ Backend
   â†“
3. Backend executes scripts/scraper.py
   â†“
4. Output: data/flipkart_boat_raw.csv
   â†“
5. User clicks "Run Phase 1"
   â†“
6. Backend executes scripts/phase1_preprocessing.py
   â†“
7. Output: data/flipkart_boat_cleaned.csv
   â†“
8. User clicks "Run Phase 2"
   â†“
9. Backend executes scripts/phase2_analysis.py
   â†“
10. Output: results/phase2_results.json
    â†“
11. Frontend fetches results via GET /api/results/phase2
    â†“
12. Charts and tables rendered with Chart.js
```

## ğŸ¨ Frontend Architecture

```
index.html
  â”œâ”€â”€ Header (title, subtitle, author)
  â”œâ”€â”€ Status Bar (3 status indicators)
  â”œâ”€â”€ Control Panel (4 action buttons)
  â”œâ”€â”€ Loading Spinner
  â””â”€â”€ Tabs Container
      â”œâ”€â”€ Overview Tab
      â”‚   â””â”€â”€ Stats Grid (4 cards)
      â”œâ”€â”€ Phase 1 Tab
      â”‚   â”œâ”€â”€ Language Chart (Pie)
      â”‚   â”œâ”€â”€ Stats Table
      â”‚   â””â”€â”€ Sample Reviews
      â”œâ”€â”€ Phase 2 Tab
      â”‚   â”œâ”€â”€ POS Chart (Bar)
      â”‚   â”œâ”€â”€ Adjectives Chart (Horizontal Bar)
      â”‚   â”œâ”€â”€ Verbs Chart (Horizontal Bar)
      â”‚   â”œâ”€â”€ NER Results (Grid)
      â”‚   â”œâ”€â”€ Topics (Cards)
      â”‚   â””â”€â”€ Word2Vec Similarities (Grid)
      â””â”€â”€ Data View Tab
          â”œâ”€â”€ Data Type Selector
          â””â”€â”€ Data Table (Dynamic)
```

## ğŸ”§ Backend API Architecture

```
Express Server (server.js)
  â”œâ”€â”€ Middleware
  â”‚   â”œâ”€â”€ CORS
  â”‚   â”œâ”€â”€ JSON Parser
  â”‚   â””â”€â”€ Static Files (public/)
  â”‚
  â””â”€â”€ Routes
      â”œâ”€â”€ GET /api/status
      â”‚   â””â”€â”€ Check if data files exist
      â”‚
      â”œâ”€â”€ POST /api/scrape
      â”‚   â””â”€â”€ Run scraper.py
      â”‚
      â”œâ”€â”€ POST /api/phase1
      â”‚   â””â”€â”€ Run phase1_preprocessing.py
      â”‚
      â”œâ”€â”€ POST /api/phase2
      â”‚   â””â”€â”€ Run phase2_analysis.py
      â”‚
      â”œâ”€â”€ GET /api/data/raw
      â”‚   â””â”€â”€ Return raw CSV as JSON
      â”‚
      â”œâ”€â”€ GET /api/data/cleaned
      â”‚   â””â”€â”€ Return cleaned CSV as JSON
      â”‚
      â””â”€â”€ GET /api/results/phase2
          â””â”€â”€ Return phase2_results.json
```

## ğŸ“Š Technology Stack

### Backend
- **Runtime**: Node.js 14+
- **Framework**: Express 4.x
- **Python**: 3.8+

### Frontend
- **HTML5**: Semantic markup
- **CSS3**: Modern styling, Flexbox, Grid
- **JavaScript**: ES6+, Async/await
- **Charts**: Chart.js 4.x

### Python Libraries
- **Web Scraping**: requests-html, beautifulsoup4
- **NLP**: nltk, gensim
- **ML**: scikit-learn
- **Data**: pandas, numpy
- **Translation**: deep-translator
- **Language Detection**: langdetect

## ğŸš€ Deployment Notes

### Development
```powershell
npm start
# Server runs on localhost:3000
```

### Production Considerations
1. Change to production-grade web server (PM2, Gunicorn)
2. Add authentication for API endpoints
3. Implement rate limiting
4. Add database for persistence
5. Use environment variables for config
6. Add logging (Winston, Morgan)
7. Implement error boundaries
8. Add data validation

## ğŸ“ˆ Performance Optimization

1. **Scraping**: Batch processing with delays
2. **Processing**: Stream large CSV files
3. **Frontend**: Lazy load charts
4. **Caching**: Cache analysis results
5. **CDN**: Use CDN for Chart.js

## ğŸ”’ Security Considerations

1. Input validation on API endpoints
2. Sanitize file paths
3. Limit file upload sizes
4. Add CSRF protection
5. Use HTTPS in production
6. Validate Python script outputs
7. Sandbox Python execution

---

**Last Updated**: November 2025
**Version**: 1.0.0
**Author**: Vipul Phatangare
